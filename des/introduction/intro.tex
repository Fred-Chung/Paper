\afterpage{\blankpage}

\chapter{Introduction}\label{s-intro}


With the development of technology, almost all information is digitized and people are  likely to be overwhelmed by a large amount of information. As a result, it is difficult to find an effective way to find, organize and understand a large amount of information{\cite{intro-back}}. Then, how to effectively obtain better information and how to automatically classify vast amounts of text data, organization and management become more challenging. Therefore, in the face of these problems and needs, the use of computers for language information processing has been extensively studied. As a research hotspot in the field of natural language processing, automatic text classification technology has been rapidly developed and widely used.


Such data is sparse and discrete which is hard for computer to process. The bag of words model are proposed for samplifying representation in natural language processing. This model can convert a sentence into a vector representation, which is a relatively straightforward method. It does not consider the order of words in the sentence, only considers the number of occurrences of words  in this sentence so that it will miss the relationshaip between each individual vocabulary and induce huge prolem of dimensional disaster. However it still a popular way in representing documents.

After representing document by matrix, scientists hope to construct a generative model to
imitate document writing. It reduces a complex precudure into some probabilistic steps and specify a sample  distribution for the topic of  document\cite{find}. This thesis will introduce  a  well-known method in natural language processing called Latent Dirichlet Allocation (LDA) which can extract  abstract themes  from a series of documents through the mechanism of generating models.


This model assumes the implicit mechanism of published documents written by humans: each document is composed of a few topics, and each topic can be composed of a few important words description. In other words, when writing an article, the topics related to our document will be first decided, then each word in text is closely related to these topics. Let's follow the generative process of LDA step by step:
\begin{itemize}
  \item For each document,a theme is extracted from the theme distribution;
  \item Extract a word from the word distribution corresponding to the above-mentioned theme;
  \item Repeat the above process until each word in the document is traversed;
\end{itemize}

This generative process is mainly compose of two distributions including the topic distribution of each docuemnt and the word distribution of each topic. In LDA, the topic distribution and word distribution are uncertain. The authors of LDA adopt the Bayesian idea that distribution of word and topic distribution should obey a prior distribution. The topic distribution and word distribution are subject to  multinomial distributions, so the topic distribution and word distribution use Dirichlet distribution as their conjugate prior distribution because the multinomial distribution and Dirichlet distribution is a conjugate structure.

As mentioned above, one commonly-used prior for topic distribution and word distribution is diriclet distribution and recently lots of distributions have been imposed on topic distribtion. For example, the correlation topic model (CTM) \cite{corr} ,which exhibits the correlation of topic by introducing logistic normal distribution. It allows each document to show different topics with different proportions and capture the relationship in groud data.    Besides, there are hierarchical document representation based on Dirichlet process and Boltzmann machines and neural network\cite{dirbn}. In Hierarchical Dirichlet Process\cite{hdp}, the author assumes that the documents can be divided into set of groups, and the latent structure to  decribe the data in the same group can be found. However the number of group is unkonwn and need to be inferred. HDP  can automatically derive the number of the optimal topics, without specifying the number of groups.

On the other hand, the total counts of document is oftenly observed by bag of models and Poisson Factor Analysis model\cite{han} proposes Beta-Gamma-Gamma-Poisson Model which can assign count of words into differenct topics. This kind of hierarchical structure is useful in sharing information between differnt topics.

Compare to numerous studies on topic distribution, research on word distribution has not received much attention. Zhao\cite{dirbn} introduces an alterative model named Dirichlet belief model to replace dirichlet prior distribution on word distribution. The output of DBN model is parameterised by Dirichlet-dsitributed hidden units which is connected with gamma weights. However,in order to obtain effieient Gibbs sampling, DirBN model back-propagates the observed information matric from output layer to upper layer by CRT\cite{crt} model which results in information decay and limination on network layer.

This paper will introduce Bounded-Dirichlet Belief Network (UDBN). One of our key contribution is the introduction of of auxiliary Poisson random variable in the layerwise connection. These auxiliary variables are appropriately designed to faciliate the model inference. Besides, the application of auxiliary circumvent the complicated bottom-up back-propagation and enable full Gibbs Sampling on inference.

The paper is organised as follows. Chapter 2  will introduce basic knowledge of Bayesian Theorem such as prior distribution, posterior distribution,common distribution used in this paper. Besides, MCMC and inference method including Gibbs Sampling also will be presented. Chapter 3 will discuss the Latent Dirichlet Allocation, including the genertive process of documents and the inference of LDA model.Beisdes, the Dirichlet belief network,which is proposed to alternate the prior distribution of word matrix also will be introduced. The last chapter in this paper proposes a new layerwise inference of Dirichlet Belief Network which introdece auxiliary Poisson random variable, further introducing the principle and the Gibbs samping process on this model.

This paper will demonstrate the modelling advantage of out layerwise  Dirichlet Belief Network in the applications of topic modelling and relational modelling.
