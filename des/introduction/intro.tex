\afterpage{\blankpage}

\chapter{Introduction}\label{s-intro}

\section{Background and Importance}
With the development of technology, alomost all information is digitized and we are  likely to be overwhelmed by a large amount of information. As a result it is difficult to find an effective way to find, organize and understand a large amount of information{\cite{intro-back}}.Then, how to effectively obtain better information and how to automatically classify vast amounts of text data, organization and management become more challenging. Therefore, in the face of these problems and needs, the use of computers for language information processing has been extensively studied. As a research hotspot in the field of natural language processing, automatic text classification technology has been rapidly developed and widely used.


Such data is sparse and discrete which is hard for computer to process.The bag of words models are proposed for samplifying representation in natural language processing.This model can convert a sentence into a vector representation, which is a relatively straightforward method. It does not consider the order of words in the sentence, only considers the number of occurrences of words  in this sentence so that it will miss the relationshaip between each individual vocabulary and induces huge prolem of dimensional disaster. However it still a popular way in representing documents.

After representing document by matrix,scientists hope to construct a generative model to
imitate document writing. It reduces a complex precudure into some probabilistic steps and specify a sample  distribution for the topic of  document\cite{find}. We will introduce  a  well-known method in natural language processing called Latent Dirichlet Allocation(LDA) which can extract  abstract "themes"  from a series of documents through the mechanism of generating models.


This model assumes the implicit mechanism of published documents written by humans: each document is composed of a few topics, and each topic can be composed of a few important words description. In other words, When writing an article, we will first decide the topics related to our document, then each word we write in text is closely related to these topics.Let's follow the generative process of LDA step by step:
\begin{itemize}
  \item For each document, we extract a theme from the theme distribution.
  \item Extracting a word from the word distribution corresponding to the above-mentioned theme.
  \item Repeat the above process until each word in the document is traversed.
\end{itemize}

This generative process is mainly compose of two distributions-the topic distribution of each docuemnt and the word distribution of each topic.In LDA, the topic distribution and word distribution are uncertain. The authors of LDA adopt the Bayesian idea that they should obey a distribution. The topic distribution and word distribution are subject to  polynomial distributions,so the topic distribution and word distribution use Dirichlet distribution as their conjugate prior distribution because the polynomial distribution and Dirichlet distribution is a conjugate structure.

As mentioned above,one commonly-used prior for topic distribution and word distribution is diriclet distribution and recently lots of distributions have been imposed on topic distribtion.For example,The correlation topic model(CTM)\cite{corr},which exhibits the correlation of topic by introducing logistic noremal distribution.It allows each document to show different topics with different proportions and capture the relationship in groud data.Besides,there are hierarchical document representation based on dirichlet process and Boltzmann machines and neural network.\cite{zhao}.In hierarchical dirichlet process,The author assume that the documents can be divided into set of groups ,and we can find the latent structure to  decribe the data in the same group. However the number of group is unkonwn and need to be inferred. HDP  can automatically derive the number of the optimal topics, without specifying the number of groups.

The total counts of document is oftenly observed by bag of models and Poisson Factor Analysis\cite{Poisson facotr analysis} proposes Beta-Gamma-Gamma-Poisson Model which can assign count into differenct topics.This kind of hierarchical structure is useful in sharing information between differnt topics.

Compare to numerous studies on topic distribution,research on word distribution has not received much attention.Zhou\cite{dirbn} introduces an alterative model named Dirichlet belief model to dirichlet prior distribution on word distribution. The output of DBN model is parameterised by Dirichlet-dsitributed hidden units which is connected with gamma weights.However,in order to obtain effieient Gibbs sampling,DirBN model back-propagates the observed information matric from output layer to upper layer by CRT\cite{} model which results in information decay and limination on network layer.

In this paper,we will introduce Bounded-Dirichlet Belief Model(UDBN).One of our key contribution is the introduction of of auxiliary Poisson random variable in the layerwise connection. These auxiliary variables are appropriately designed to faciliate the model inference.Besides,the application of auxiliary circumvent the complicated bottom-up back-propagation and enable full Gibbs Sampling on inference.

The paper is organised as follows.In Chapter 2 we will introduce basic knowledge of Bayesian Theorem,common distribution used in this paper.Besides,inference method including Gibbs Sampling also will be presented.In Chapter 3,we will discuss the Latent Dirichlet Allocation.
In 

% Mention of previous work on the subject
% Statemeng of the importance of the subject

% 引言部分的第一段需要给出研究领域的大背景及其重要性所在。这个大背景勾勒出该领域科研成果从古至今的一个走向或者趋势 (what is known)，为接下来本论文课题的发展生长提供温床。这部分内容的展开一定要引用该领域前人、大牛的经典文献或者奠基性著作，体现你对于该学科的一个总体把握是全面且客观的。那如何从这个大背景引入到论文的课题呢？当我们从该学术领域的大趋势逐渐缩小范围时，只需把其中
?
% 摆出该研究领域的一个概况之后，就要顺理成章地指出哪些是我们还未涉及的领域（或是没有研究透彻的问题）。当然这样的问题有千千万万，此处不能全部罗列，必要要对应着你的论文课题来谈。直白地说，就是你论文课题研究的什么，此处就针对性地写“这个课题尚未被太多科研者涉足”云云。

\section{topic of research paper}
Announce the research topic/question being addressed in research paper

% 既然研究空白已近在眼前，那么对应的研究课题便要紧随其后。接下来就是你理直气壮陈述该论文主题的时刻，此时需要注意与研究空白的呼应，即在摆出课题是什么的基础之上更近一层，简要分析这个课题是

\section{Highlight the approach and principal finding}

DEscrition of your paper approach and why you chose it

brief summary of your major findings

\section{Organization of this paper}
