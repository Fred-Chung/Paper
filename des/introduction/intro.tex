\afterpage{\blankpage}

\chapter{Introduction}\label{s-intro}


With the development of technology, alomost all information is digitized and we are  likely to be overwhelmed by a large amount of information. As a result it is difficult to find an effective way to find, organize and understand a large amount of information{\cite{intro-back}}.Then, how to effectively obtain better information and how to automatically classify vast amounts of text data, organization and management become more challenging. Therefore, in the face of these problems and needs, the use of computers for language information processing has been extensively studied. As a research hotspot in the field of natural language processing, automatic text classification technology has been rapidly developed and widely used.


Such data is sparse and discrete which is hard for computer to process.The bag of words models are proposed for samplifying representation in natural language processing.This model can convert a sentence into a vector representation, which is a relatively straightforward method. It does not consider the order of words in the sentence, only considers the number of occurrences of words  in this sentence so that it will miss the relationshaip between each individual vocabulary and induces huge prolem of dimensional disaster. However it still a popular way in representing documents.

After representing document by matrix,scientists hope to construct a generative model to
imitate document writing. It reduces a complex precudure into some probabilistic steps and specify a sample  distribution for the topic of  document\cite{find}. We will introduce  a  well-known method in natural language processing called Latent Dirichlet Allocation(LDA) which can extract  abstract "themes"  from a series of documents through the mechanism of generating models.


This model assumes the implicit mechanism of published documents written by humans: each document is composed of a few topics, and each topic can be composed of a few important words description. In other words, When writing an article, we will first decide the topics related to our document, then each word we write in text is closely related to these topics.Let's follow the generative process of LDA step by step:
\begin{itemize}
  \item For each document, we extract a theme from the theme distribution.
  \item Extracting a word from the word distribution corresponding to the above-mentioned theme.
  \item Repeat the above process until each word in the document is traversed.
\end{itemize}

This generative process is mainly compose of two distributions-the topic distribution of each docuemnt and the word distribution of each topic.In LDA, the topic distribution and word distribution are uncertain. The authors of LDA adopt the Bayesian idea that they should obey a distribution. The topic distribution and word distribution are subject to  multinomial distributions,so the topic distribution and word distribution use Dirichlet distribution as their conjugate prior distribution because the polynomial distribution and Dirichlet distribution is a conjugate structure.

As mentioned above,one commonly-used prior for topic distribution and word distribution is diriclet distribution and recently lots of distributions have been imposed on topic distribtion.For example,The correlation topic model(CTM) \cite{corr} ,which exhibits the correlation of topic by introducing logistic noremal distribution.It allows each document to show different topics with different proportions and capture the relationship in groud data.Besides,there are hierarchical document representation based on dirichlet process and Boltzmann machines and neural network.\cite{zhao}.In hierarchical dirichlet process,The author assume that the documents can be divided into set of groups ,and we can find the latent structure to  decribe the data in the same group. However the number of group is unkonwn and need to be inferred. HDP  can automatically derive the number of the optimal topics, without specifying the number of groups.

The total counts of document is oftenly observed by bag of models and Poisson Factor Analysis model\cite{han} proposes Beta-Gamma-Gamma-Poisson Model which can assign count into differenct topics.This kind of hierarchical structure is useful in sharing information between differnt topics.

Compare to numerous studies on topic distribution,research on word distribution has not received much attention.Zhou \cite{dirbn} introduces an alterative model named Dirichlet belief model to replace dirichlet prior distribution on word distribution. The output of DBN model is parameterised by Dirichlet-dsitributed hidden units which is connected with gamma weights.However,in order to obtain effieient Gibbs sampling,DirBN model back-propagates the observed information matric from output layer to upper layer by CRT\cite{crt} model which results in information decay and limination on network layer.

In this paper,we will introduce Bounded-Dirichlet Belief Model(UDBN).One of our key contribution is the introduction of of auxiliary Poisson random variable in the layerwise connection. These auxiliary variables are appropriately designed to faciliate the model inference.Besides,the application of auxiliary circumvent the complicated bottom-up back-propagation and enable full Gibbs Sampling on inference.

The paper is organised as follows.In Chapter 2 we will introduce basic knowledge of Bayesian Theorem such as prior distribution ,posterior distribution,common distribution used in this paper.Besides,MCMC and inference method including Gibbs Sampling also will be presented.In Chapter 3,we will discuss the Latent Dirichlet Allocation,including the genertive process of documents and the inference of LDA model.Beisdes, we will introduce the Dirichlet belief network,which is proposed to alternate the prior distribution of word matrix.In the last chapter ,we propose a new layerwise inference of Dirichlet Belief Network which introdece auxiliary Poisson random variable, further introducing the principle and the Gibbs samping process on this model.

we demonstrate the modelling advantage of out layerwise  Dirichlet Belief Network in the applications of topic modelling and relational modelling.
