\section{Markov Chain Monte Carlo and Gibbs Sampling}\label{bases}

\subsection{Monte Carlo Method}
The Monte Carlo method is a calculation method. The principle is to understand a system through a large number of random samples, and then get the value to be calculated.

It is very powerful and flexible and  quite easy to understand and  implement. For many problems, it is often the simplest calculation method, and sometimes even the only feasible method.
As a random sampling method, Markov Chain Monte Carlo has a wide range of applications in the fields of machine learning, deep learning, and natural language processing, and is the basis of many complex algorithms.

The early Monte Carlo methods were designed to solve some summation or integration problems that are not very easy to solve. Such as :
\[
Y = \int_a^b f(x)

\]

We can get the answer by Newton-Leibniz formula if the function is simple enough.However,
In most cases, it is difficult to find the original function of $f(x)$. Of course, we can use the Monte Carlo method to simulate the conversion approximation

Then we can sample$n$ values ​​in the $[a,b]$, interval: $x_0,x_1,\dots,x_n$, and use their average values ​​to represent all $f(x)$ values ​​in the $[a,b]$ interval. So our approximate solution to the definite integral above is:
\[
  Y = \frac{b-a}{n}\sum^n f(x_i)
\]

The above method has an implicit assumption that the distribution follows a uniform distribution from a to b, but the actual situation is will subject to  various types of distribution.We can improve our method as follows
\begin{eqnarray*}
Y &=& \int_a^b f(x) \\
  &=& \int_a^b \frac{f(x)}{p(x)}p(x) \\
  &=& \frac{1}{n} \sum^n \frac{f(x_i)}{p(x_i)}\\
\end{eqnarray*}

Our question now turns to how to find the distribution of $x$
\subsection{Sampling Method}
The key to the Monte Carlo method is to get the probability distribution of $x$. If the probability distribution of $x$ is found, we can sample n sample sets based on this probability distribution based on the probability distribution and bring it into the Monte Carlo summation formula to solve.

For the common uniform distribution-$uniform (0,1)$,It  is very easy to get samples, generally through the linear congruential generator that can easily generate between (0,1) Pseudo-random number samples. For other common probability distributions, whether they are discrete distributions or continuous distributions, their samples can be infered by $uniform (0,1)$ sample conversion.

Assuming that $x$ is a continuous random variable, subject to a random distribution$f(x)$, its cumulative distribution function is $F(X)$.We assume that $Y = F(X)$ is subject to 0-1 uniform distribution and $F^{-1}(Y)$ have same distribution with $X$.For example:
PDF of Expoential distribution:
\[
  f(x) = \lambda e^{\lambda*x}
\]
CDF of Expoential distribution:
\[
  F(x) = 1- exp^{\lambda*x}
\]
Inverse sampling inference:

\begin{eqnarray*}
u & \sim & Uniform(0,1) \\
F(F^{-1}(Y)) &=& 1- exp^{\lambda*F^{-1}(Y)} = u \\
F^{-1}(Y) &=&-\frac{\log(1-u)}{\lambda}
\end{eqnarray*}

But many times, our probability distribution is not a common distribution, which means that we can't easily get a sample set of these unusual probability distributions. We need to Reject-Sampling method.

The basic idea of Reject-Sampling is to cover the smaller probability distribution with a larger probability distribution. This larger probability distribution is more easier to sample (standard distribution), and at the same time accept this  sample with a certain probability. The sample can be seen as a sample from the "smaller probability distribution"


We can solve some cases  by Reject-Sampling when the probability distribution is not common.However,in the case of high dimensions, Rejection Sampling will have two problems. The first is that the proposal  distribution $q$ is difficult to find, and the second is that it is difficult to determine a reasonable value of $k$. These two problems will lead to a high rejection rate and an increase in useless calculations.


From the probability density function $p(X)$ of a known distribution, we want to get  samples $X$that subject to this distribution.
\begin{itemize}
  \item Sample $Y$ from proposal distribution $p(x)$
  \item Sample $U$ from uniform(0-1) distribution
  \item if $U<\frac{f(Y)}{c*g(Y)}$,then we accept Y,else continue.
\end{itemize}


\subsection{Markov Chain}
This is a random process from state to state in the state space. This process requires the "no memory" attribute: the probability distribution of the next state can only be determined by the current state, and the events before it in the time series have nothing to do with it. This special type of "no memory" is called the Markov attribute. Markov chains have many applications as statistical models of actual processes


In order to obtain a theoretical result, let's look at a smaller example (this will facilitate our subsequent calculation demonstration), assuming that in a region, people either live in the city or live in the countryside. The matrix below tells us some laws  of population migration. For example, the first column of row 1 indicates that 90 percent of the population currently living in cities will choose to continue to live in cities next year and 80 \% of people who live in country will continue to stay in country.
$$
H_x={
\left[ \begin{array}{ccc}
0.9 & 0.1 \\
0.2 & 0.8\\
\end{array}
\right ]}
$$

As a simple start, try to calculate the odds of people living in cities today who will live in the countryside after 2 years. Analysis shows that 90 of people currently living in the city will continue to choose to live in the city after 1 year, and 10 will move to the country. Then another year passed, and 10 of those who chose to stay in the city last year moved to the countryside. And 98 of those who moved to the village last year will choose to stay in the village. This analysis process is shown in the figure below, and finally the probability that people who live in the city will live in the countryside after 2 years = 0.90×0.10+0.10×0.98


In fact, you will find that our calculation process is to do the square of the matrix. As shown in the following figure, you will find that the calculation in the second row and first column of the result matrix is ​​performing the operation that is processed above. On this basis, we can also continue to calculate the situation after n years, that is, the result of calculating the self-multiplication of matrix A n time

The algorithm is as follows:Enter the Markov chain state transition matrix , set the state transition times threshold , the required number of samples ;Sampling from any simple probability distribution to get the initial state value ;sample  from the conditional probability distribution ;The sample set is the corresponding sample set that meets our stationary distribution.


If it is assumed that we can obtain the Markov chain state transition matrix corresponding to the stationary distribution of the samples we need to sample, then we can use the Markov chain sampling to obtain the sample set we need, and then perform Monte Carlo simulation.

But an important question is, given a random distribution at will, how to get the Markov chain state transition matrix $P$ corresponding to it?

\subsection{MCMC Sampling and M-H Sampling}
Mostly,
Target stationary distribution $\pi(x)$and a certain Markov chain state transition matrix $Q$ does not satisfy the detailed stationary condition:
\[
  \pi(i)Q(i,j) \neq \pi(j)Q(j,i)
\]


We introduce a $\alpha(i,j)$ so that the above formula can take the equal sign.
\[
  \pi(i)Q(i,j)\alpha(i,j) \eq \pi(j)Q(j,i)\alpha(j,i)
\]

But how can we get the $\alpha$,by symmetry:
\begin{eqnarray*}
\alpha(i,j) &=& \pi(j)Q(j,i)\\
\alpha(j,i) &=& \pi(i)Q(i,j)
\end{eqnarray*}


$\alpha$ is generally called acceptance rate, and the value is between $[0～1]$, which can be understood as a probability value. This is much like accept-reject sampling, where a common distribution is obtained through a certain acceptance-rejection probability, and here is a common Markov chain state transition matrix $Q$ through a certain acceptance-rejection probability Obtaining the target transition matrix $p$, the two solutions to the problem are similar.

MCMC sampling algorithm is as follows:
\begin{enumerate}
  \item Enter any given Markov chain state transition matrix $Q$, target stable distribution $\pi(x)$, set the threshold of state transition times $n1$, the number of required samples $n2$;
  \item Get the initial state value $x_0$ from any simple probability distribution;
  \item For $t$ =0 in $n_1+n_2-1$
    \begin{itemize}
      \item Get the sample value $x_*$ from the conditional probability distribution $Q(x_*|x_0)$.
      \item Sample $U$ from uniform distribution.
      \item if $u<\pi(x_*)Q(x_*|x_0)$，then accept x_*.
    \end{itemize}
\end{enumerate}


But this sampling algorithm is still more difficult to apply in practice, because in the third step, because accept rate $\alpha$ may be very small, such as 0.1, most of our sampled values ​​are rejected and the sampling efficiency is very low. It is possible that we have sampled millions of Markov chains and have not yet converged, that is, the above $n_1$ should be  very large, which is unacceptable.

At this time, it is our Metropolis-Hastings sampling Method.M-H sampling solves the problem of low MCMC sampling acceptance rate in the previous section.

We can expand both sides of \[
  \pi(i)Q(i,j)\alpha(i,j) ＝ \pi(j)Q(j,i)\alpha(j,i)
\], and at this time the detailed stationary condition is also satisfied. We expand the equation by $C$ times to make $c\alpha(ij)$ (accurately, the maximum expansion of both sides is 1), so that we can Improves the acceptance rate of jumps in sampling, so we can take:
\[
  \alpha = min{\frac{Q(j,i)\alpha(j,i)}{Q(i,j)\alpha(i,j)},1}
\]

In the era of big data, M-H sampling faces two major challenges:
\begin{enumerate}
  \item Our data features are very many, M-H sampling due to the existence of the acceptance rate calculation , the calculation time required in high dimensions is very considerable, and the algorithm efficiency is very low. At the same time, $\alpha$　is generally less than 1. Sometimes it is hard to calculate but it is rejected. Can it be done without refusing to transfer?
  \item Due to the large feature dimension, it is often difficult to find the joint distribution of each feature dimension of the target, but it is convenient to find the conditional probability distribution between each feature. At this time, can we only have convenient sampling in the case of conditional probability distribution between various dimensions?
\end{enumerate}

\subsection{Gibbs Sampling}
Gibbs Sampling Method is a one special MCMC technique used for sampling variales in large dimensions by sampling each variable from its conditional distribution iterative.

Starting from a two-dimensional data distribution, assuming that $\pi(x_1,x_2)$ is a two-dimensional joint data distribution, observe the first two points with the same feature size$A(x_1^{(1)},x_2^{(1)}$ and $B(x_1^{(1)},x_2^{(2))}$.For example:
\[
  \pi(x_1^{(1)},x_2^{(1)}\pi(x_2^{(2)}|x_1^{(1)}) = \pi(x_1^{(1)})\pi(x_2^{(2)}|x_1^{(1)})\pi(x_2^{(1)}|x_1^{(1)}）
\]

\[
  \pi(x_1^{(1)},x_2^{(1)}\pi(x_2^{(1)}|x_1^{(1)}) = \pi(x_1^{(1)})\pi(x_2^{(2)}|x_1^{(1)})\pi(x_2^{(1)}|x_1^{(1)}
\]

Since the right sides of the two formulas are equal, we have:
\[
  \pi(x_1^{(1)},x_2^{(1)})\pi(x_2^{(2)}|x_1^{(1)}) =   \pi(x_1^{(1)},x_2^{(1)})\pi(x_2^{(1)}|x_1^{(1)})
\]

Observing the above detail balance formula , we find that on the straight line of $x_1 = x_1^{(1)}$, if the conditional probability distribution $\pi(x_2|x_1^{(1)})$ is used as the state transition probability of the Markov chain, the transition between any two points meets Detail balance　conditions! In the same way, on the straight line of $x_2 = x_2^{(1)}$, if the conditional probability distribution $\pi(x_1|x_2^{(1)})$  is used as the state transition probability of the Markov chain, the transition between any two points also meets the detail balance condition.


With the  transition matrix, we can infer a two-dimensional Gibbs sample, which requires a conditional probability between two dimensions. The algorithm is as follows:
\begin{enumerate}
  \item Given stationary distribution $\pi(x_1,x_2)$,Set the threshold value of the number of state transitions $n_1$, the number of required samples $n_2$.
  \item Randomly initialize values $x_1^{(1)}$ and $x_2^{(1)}$.
  \item for t in [0,n1+n2-1]:
      \begin{itemize}
        \item Get sample $x_2^{(t+1)}$ from conditional distribution $p(x_2|x_1^t)$
        \item Get sample $x_1^{(t+1)}$ from conditional distribution $p(x_1|x_2^t)$
      \end{itemize}
\end{enumerate}
