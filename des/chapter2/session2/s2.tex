\section{Basic distribution}

\subsection{Beta distribution}
The beta distribution can represent the probability distribution of a probability. When the specific probability of a thing is unknown, it can be regards as  the probability of the occurrence of all probabilities.

The definition of Beta Function is as follow:
For each positive $\alpha$ and $\beta$:
$$P(x|\alpha,\beta) = \frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1} $$
%
% The function B is called beta function.
$$B(\alpha,\beta) = \frac{\Gamma(\alpha)  \Gamma(\beta)}{\Gamma{\alpha+\beta}}$$

For example, carry out $N$ times of Bernoulli test, the probability of success of the test p is subject to a prior probability density distribution $Beta(\alpha,\beta)$, and the test result appears $K$ times of success of the test. The posterior probability density distribution of the probability p of the test success is $Beta(\alpha + K,\beta+N-K)$. Prove:

Prior distribution:
\[
  f(p|\alpha,\beta) &=& \frac{\Gamma(\alpha)  \Gamma(\beta)}{\Gamma{\alpha+\beta}}x^{\alpha-1}(1-x)^{\beta-1} \
\]
Likelihood Function:
\[
  f(n_1,n_2,\ndot,N|p) = p^{K}(1-p)^{N-K}
\]
Posterior distribution:
\begin{eqnarray*}
  f(p|n_1,n_2,..N,\alpha,\beta) &=& \frac{f(n_1,n_2,\ndot,N|p)f(p|\alpha,\beta)}
  {f(n1,n2,...N,\alpha,\beta)}
\end{eqnarray*}

Given that:
\begin{eqnarray*}
  f(n1,n2,...N,\alpha,\beta) &=& \int_p f(n_1,n_2,\ndot,N|p)f(p|\alpha,\beta) \\
  & = &\frac{1}{B(\alpha,\beta)} \int_p p^{\alpha+K-1}(1-p)^{\beta+N-K-1} \\
  &=&  \frac{B(\alpha+k,\beta+N-K)}{B(\alpha,\beta)}
\end{eqnarray*}
\begin{eqnarray*}
  f(n_1,n_2,\ndot,N|p)f(p|\alpha,\beta) &=& \frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1} * \frac{f(n_1,n_2,\ndot,N|p)f(p|\alpha,\beta)}
  {f(n1,n2,...N,\alpha,\beta)} \\
  &=& \frac{1}{B(\alpha,\beta)}p^{\alpha+K-1}(1-p)^{\beta+N-K-1}
\end{eqnarray*}

So that:
\begin{eqnarray*}
  f(p|n_1,n_2,..N,\alpha,\beta) &=& \frac{1}{\alpha+K-1}(1-p)^{\beta+N-K-1}p^{\alpha+K-1}(1-p)^{\beta+N-K-1} \\
  &=& Beta(\alpha + K,\beta+N-K)
\end{eqnarray*}


\subsection{Dirichlet distribution}
Dirichlet distribution is a generalization of the beta distribution in high dimensions. So the definition of  probility density function is:
\[
  f(x_1,x_2,\ndots,x_n|\alpha_1,\ndots,\alpha_n) = \frac{1}{B(\alpha)} \prod_{i=1}^n x_i^{\alpha_i-1}
\]
where the parameter $\alpha_1,\dots,\alpha_n > 0$ and $\sum_{i=1}^n =1$

Besides, the normalizing constant $B(\alpha)$ is the multivariate beta function:
\[
  B(\alpha) = \frac{\prod_{i=1}^n \Gamma(\alpha_i) }{\Gamma(\sum_{i=1}^k \alpha_i)}
\]

For example,$K$ times of test will be carried out in this experiment and each event will get $n$ results and the probability of result is subject to a prior probability density distribution $Dirichlet(\alpha_1,\ndots,\alpha_N)$,
the experiment will record the number of occurrences of each situation $[x_1,x_2,\ndots,x_n]$, and the posterior probability density distribution of the probability of the test is subject to $Dirichlet(\alpha_1 + x_1,\ndots,\alpha_n+x_n)$. Prove:

Prior distribution:
\[
  f(p_1,p_2,\ndots,p_n|\alpha_1,\dots,\alpha_n) = \frac{1}{B(\alpha)} \prod_{i=1}^n x_i^{\alpha_i-1}
\]

likelihood Function:
\[
  f(x_1,x_2,\ndot,x_n|k,p_1,p_2,\dots,p_n) &=&  \frac{k!}{x_1!,\ndots,x_n} \prod_{i=1}^n p_i^{x_i}
\]
Posterior distribution:
\begin{eqnarray*}
  f(p_1,p_2,\dots,p_n|k,x_1 \dots x_n,\alpha_1 \dots \alpha_n) &=& \frac{f(x_1\ndot,x_n|k,p_1\dots p_n)*f(p_1 \ndots p_n|\alpha_1 \dots \alpha_n)}{\int_x f(k,x_1 \dots x_n,\alpha_1,\dots,\alpha_n)}
\end{eqnarray*}

Given that:
\begin{eqnarray*}
  \int_x f(k,x_1 \dots x_n,\alpha_1,\dots,\alpha_n) &=& \int_p f(x_1\ndot,x_n|k,p_1\dots p_n)*f(p_1 \ndots p_n|\alpha_1 \dots \alpha_n) \\
  & = &\frac{k!}{x_1!,\ndots,x_n}  \frac{1}{B(\alpha)}  \int_p \prod_{i=1}^n p_1^{\alpha_1+x_1} \\
  &=&  \frac{k!}{x_1!,\ndots,x_n!}  \frac{B(\alpha + x)}{B(\alpha)}
\end{eqnarray*}

\begin{eqnarray*}
  f(x_1\ndot,x_n|k,p_1\dots p_n)*f(p_1 \ndots p_n|\alpha_1 \dots \alpha_n) &=& \frac{k!}{x_1!,\ndots,x_n} \prod_{i=1}^n p_i^{x_i} \frac{1}{B(\alpha)} \prod_{i=1}^n p_i^{\alpha_i-1} \\
  &=& \frac{k!}{x_1!,\ndots,x_n!}  \frac{1}{B(\alpha)} \prod_{i=1}^n p_i^{x_i + \alpha_i-1} \\
\end{eqnarray*}

So that:
\begin{eqnarray*}
f(p_1,p_2,\dots,p_n|k,x_1 \dots x_n,\alpha_1 \dots \alpha_n) &=& \frac{1}{B(\alpha + x)} \prod_{i=1}^n p_i^{x_i + \alpha_i-1}\\
  &=& Dirichlet(x + \alpha)
\end{eqnarray*}







\subsection{Poisson and Multinomial distribution}

The Poisson process is a continuous time random process with discrete values. Poisson process is used to describe the number of occurrences of events in a period of time, so it is called a counting process. Given the time-dependent random variable as $N_t$, poisson distribution is used to describe the number of occurrences of an event from time 0 to time $t$.


In short, according to the average number of occurrences of a random event in a certain period of time or in a certain space in the past, poisson distribution can predict the probability that the random event will occur $k$ times in the same long time or the same large space in the future. Its probability mass function is:
\[
  P(X=K|\lambda) = \frac{e^{-\lambda} \lambda^k}{k!}
\]
st.
\begin{itemize}
  \item
$\lambda$ is the average number of random events in a certain period of time or in a certain space in the past.
\item$ k>0$.
\end{itemize}


Multinomial Distribution is a generalization of Binomial Distribution. Bernoulli experiments stipulates that there are only two results for Bernoulli test. However, multinomial distribution assumes that there are as many as $m$ results for each experiment, and the sum of probability is equal to 1, then the probability that one of the results happens $X$ times subject to  a multinomial distribution.

\[
  f(x_1,x_2,\ndot,x_n|k,p_1,p_2,\dots,p_n) &=&  \frac{k!}{x_1!,\ndots,x_n} \prod_{i=1}^n p_i^{x_i}
\]

where $\sum_{i=1}^n = k$.

There is some transformation betweem poisson distribution and multinomial distribution\cite{han}. Assuming that:
\[
  x_{pi} = \sum_{k=1}^K x_{pik}
\]
\[
  x_{pik} \sim  Pois(\alpha_{pk}\theta_{ki})
\]


An equivalent augmentation is\cite{bunt}:
\[
  x_{pi} \sim Pois(\sum_{k=1}^K \alpha_{pk}\theta_{ki})
\]
\[
  \eta_{pik} = \frac{\alpha_{pk}\theta{ki}}{\sum_{k=1}^K \alpha_{pk}\theta_{ki}}
\]
\[
  f(x_{pi1} \dots x_{piK}) \sim Mult(x_{pi}|\eta_{pi1} \dots \eta_{piK})
\]
