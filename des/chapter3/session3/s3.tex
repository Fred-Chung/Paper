\section{Dirichlet belief networks}
As can be seen from the above, the bright distribution of topics and word matrices obey Dirichlet distribution, which is an effective assumption, but it also brings many limitations. Recently,people introduce different generative models
\subsection{Research on Topic Distribution}

The LDA model assumes that the topics are independent of each other, however, this assumption
It is very inconsistent with the actual data set. To overcome this defect, Blei
In 2006, a related topic model (Correlated Topic Model,
CTM)\cite{A CORRELATED TOPIC MODEL OF SCIENCE 1}, the model extracts the topic from the Logistic Normal distribution, overcoming the disadvantage that
 LDA model cannot extract relatation of  information between texts.This model mentioned
above have been successfully applied in extracting scientific subjects and image extraction.

Another well-known research on topic-distribution is Hierarchical Dirichlet processes\cite{Teh}.
Based on the deformation of Dirichlet Process,
HDP is A non-parametric Bayesian model that can automatically train the most suitable from the document set
Appropriate number of topics K. Nonparametric characteristics of HDP through Dirichlet process
Solve the problem of selecting the number of topics in LDA, and the experiment confirms  that
The optimal number of topics selected by HDP is equal to the optimal number of topics selected based on perplexity
.
LDA model assumes that topic of each word is subject to multinomial distribution and the document is converted into count  matrix by Bag of Word model.Poisson Factor \cite{Beta-Negative Binomial Process and Poisson Factor Analysis} Analysis introduces poisson distribution.
\[
  x_pi = \sum_{k=1}^{K} x_{pik}
\]
\[
  x_{pik} \sim Pois(\gamma_{pk}\theta_{ki})
\]
and topic-distribution is subject to gamma distribution:
\[
  \theta_{ki} \sim Gamma(r_k,\frac{p_k}{1-p_k})
\]

\subsection{Introduction of DIRBN}
Compared to considerable researchs  topic model,The word model has not been fully studied and The Dirichlet Belief Network (DBN)\cite{Zhao} introduces a deep generative model where each layer is weighted by sets of  topics.

Different from single layer dirichlet,DirBN model proposed a multi-layer Dirchlet layer generative process on word-distribution.The latent distri-
butions in each layer of DBN are generated as Dirichlet
random variables and can thus be interpreted as categorical
distributions.Then  the hidden units are connected with gamma-distributed weights.

\begin{figure}[htbp]
% \centering % 图片居中
\includegraphics[width = 13cm]{dirbn.png}
\caption{The caption of this figure.}
\label{fig:figure1label}
\end{figure}

In comparison to existing deep generative models,DirBN have better Interpretability on topics annd higher modelling accuracy.However,the current formulation of DBN odel suffers from decay during information passing.

DBN’s deep architecture is currently limited to
only a few layers. In order to obtain efficient Gibbs sam-
pling, DBN back-propagates the observed information from
the output layer to each hidden layer. The cost of the in-
formation back-propagation is that the information would
decay in a O(log) rate on passing through one layer to its
upper layer \cite{Zhou}. Therefore, little informa-
tion might be available after a few layer back-propagations
\subsection{Inference of DirBN}
The training of DirBN model introduce several data augmentation techniques in the inference of latent variables.
Given topic-doc matrix $\theta$ and topic-word matrix $\phi$we can sample the topic of each word ini each document.Then we get the topic-word count matrix $x_{k_1}^{(1)} = [x_{1k_1}^{(1)},\dots,x_{Vk_1}^{(1)}]$ which is also the input count vector of DirBN inference process.
The process of Inference involve two key parts:
\begin{enumerate}
  \item Propagating the input count vector from
Bottom to top.Due to the inconjugate property between these latent distri-butions, direct efficient Gibbs sampling over these random variables is difficult. Insteadchoose to first back propagates the observed counting information into each layer and then proceed forward variable Gibbs sampling.
\item updating latent parameters from top to bottom.After back propagatation of latent count,update the variables by conjugate posterior distribution.

\end{enumerate}
